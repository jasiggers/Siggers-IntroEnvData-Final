--- 
title: "Intro to Environmental Data Science Final"
author: "Alex Siggers"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: rstudio/Siggers-IntroEnvData-Final
description: "This is a culmination of assignments completed throughout the semester"
---



<!--chapter:end:index.Rmd-->

---
title: "SOCR 580 Assignment 1"
author: "Alex Siggers"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(dataRetrieval)
library(dygraphs)
library(xts)
```

#Chapter 1: Introduction to RMarkdown through the Poudre

# Methods

The Poudre River at Lincoln Bridge is:

  - Downstream of only a little bit of urban stormwater

  - Near Odell Brewing CO
  
  - Near an open space area and the Poudre River Trail
  
  - **Downstream of many agricultral diversions**


## SiteDescription

![](https://waterdata.usgs.gov/nwisweb/local/state/co/text/pics/06752260big.jpg)

## Data Acquisition and Plotting tests

### Data Download


```{r downloader}

q <- readNWISdv(siteNumbers = '06752260',
                parameterCd = '00060',
                startDate = '2017-01-01',
                endDate = '2022-01-01') %>%
  rename(q = 'X_00060_00003')


```



### Static Data Plotter


```{r, warning = FALSE, fig.width = 8, fig.height = 5}

ggplot(q, aes(x = Date, y = q)) + 
  geom_line() + 
  ylab('Q (cfs)') + 
  ggtitle('Discharge in the Poudre River, Fort Collins')

```


### Interactive Data Plotter


```{r}

q_xts <- xts(q$q, order.by = q$Date)


dygraph(q_xts) %>%
  dyAxis("y", label = "Discharge (cfs)")
```



# Assignment. 

## DyGraph example. 

```{r}

dygraph(q_xts) %>%
  dyAxis("y", label = "Discharge (cfs)") %>%
  dyAxis("x", label = "Time")%>%
   dyOptions(drawPoints = TRUE, pointSize = 2)
```


## Poudre Paragraph

Colorado's Front Range would not be the same without the critical flow of the Cache la Poudre (*POO-der*) River. The headwaters begin high in the Rocky Mountains, initiating a 125-mile journey (https://poudreheritage.org/wp-content/uploads/poudremapgeneral-1.jpg) that drops an astounding **7,000 feet** of elevation on its way to the grasslands of northeastern Colorado. As with many of Colorado's major rivers, the 'Poudre' has played a crucial role in the development of agriculture, economy, recreation, and even water laws- more on this here (https://poudreheritage.org/water-war-and-law/). A ride up the Poudre River Canyon from Fort Collins will quickly reveal the impact that the river has had on the geology of the region. **Steep** drop-offs (https://coloradoencyclopedia.org/sites/default/files/Cache_la_Poudre_River_02.jpg) play host to numerous ecosystem types, and consequently, a variety of organisms including mountain mahogany, sagebrush, bighorn sheep, and even **mountain lions**. The importance of this river is difficult to overstate, so conservation of this beauty must remain a focal point of local communities' agendas. 



<!--chapter:end:1-Rmarkdown-examples.Rmd-->

---
title: "Hayman Fire Recovery"
author: "Alex Siggers"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, warning=F,message=F,echo=F}
library(tidyverse)
library(tidyr)
library(ggthemes)
library(lubridate)
library(ggpubr)

knitr::opts_knit$set(root.dir='..')
```

#Chapter 2: Hayman Fire Recovery

```{r dataread, warning=F, message=F, echo=F}
#Reading in files

files <- list.files('./data/2-fire-data-wrangle',full.names=T)


#Read in individual data files
ndmi <- read_csv(files[1]) %>% 
  rename(burned=2,unburned=3) %>%
  mutate(data='ndmi')


ndsi <- read_csv(files[2]) %>% 
  rename(burned=2,unburned=3) %>%
  mutate(data='ndsi')

ndvi <- read_csv(files[3])%>% 
  rename(burned=2,unburned=3) %>%
  mutate(data='ndvi')

# Stack as a tidy dataset
full_long <- rbind(ndvi,ndmi,ndsi) %>%
  gather(key='site',value='value',-DateTime,-data) %>%
  filter(!is.na(value))

```


1) What is the correlation between NDVI and NDMI?

```{r, echo=FALSE, warning=F, message=F}

#Creating & Plotting Summer dataset

full_wide <- spread(data=full_long,key='data',value='value') %>%
  filter_if(is.numeric,all_vars(!is.na(.))) %>%
  mutate(month = month(DateTime),
         year = year(DateTime))

summer_only <- filter(full_wide,month %in% c(6,7,8,9))

ggplot(summer_only,aes(x=ndmi,y=ndvi,color=site)) + 
  geom_point() + 
  geom_smooth(method="lm") +
  theme_few() + 
  scale_color_few() + 
  theme(legend.position=c(0.8,0.8))
```

There appears to be a consistent positive correlation between NDMI (vegetation water content) and NDVI (vegetative cover) in the summer months at all sites on average. This correlation is stronger in the burned sites, which may be driven largely by post-fire succession. 


2) What is the correlation between average NDSI (normalized
 snow index) for January - April and average NDVI for June-August?
 
```{r,  echo=FALSE, warning=F, message=F}

## Generate ndsi winter dataset w/ averages ##

WinterNDSI= full_long %>%
  filter(!is.na(value)) %>%
  pivot_wider(names_from = c(data),
              values_from = c(value)) %>%
  mutate(year= year(DateTime),
         month= month(DateTime)) %>%
  filter(month %in% c(1,2,3,4)) %>%
  group_by(site, year) %>%
  summarise(MeanNDSI = mean(ndsi))

## Generate ndvi summer dataset w/ averages ##

SummerNDVI= full_long %>%
  filter(!is.na(value)) %>%
  pivot_wider(names_from = c(data),
values_from = c(value)) %>%
  mutate(year= year(DateTime),
         month= month(DateTime)) %>%
  filter(month %in% c(6,7,8)) %>%
  group_by(site, year) %>%
  summarise(MeanNDVI = mean(ndvi))

## Combine datasets ##

Q2 = inner_join(WinterNDSI,
                SummerNDVI)

## Plotting Data ##

Q2 %>%
  filter_if(is.numeric,all_vars(!is.na(.))) %>%
ggplot(aes(x=MeanNDSI, y=MeanNDVI))+
  geom_point()+
  geom_smooth(method="lm")+
   scale_color_few() +  
  labs(title="Influence of Snow", x="Average NDVI", y="Average NDSI") +
  theme(plot.title = element_text(hjust = 0.5))
  
```
 
After taking averages and fitting to a linear model, it appears that there is a slightly positive correlation. This leads me to believe that increased winter snowfall may encourage vegetative growth in the concurrent summer months. 


3) How is the snow effect from question 2 different between pre- and post-burn
and burned and unburned? 

```{r, echo=FALSE, warning=F, message=F}

## Creating Pre & Post-burn datasets ##

Q3Pre= Q2 %>%
  filter(year %in% c(1984:2002))

Q3Post= Q2 %>%
  filter(year %in% c(2003:2019))

## Plotting Averages ##

ggplot(data=Q3Pre, aes(x=MeanNDSI, y=MeanNDVI, color=site)) +
  geom_point() +
  geom_smooth(method="lm") +
  theme_few() + 
  scale_color_few() + 
  theme(legend.position="bottom") +
  labs(title="Pre-Burn", x="Average NDSI", y="Average NDVI") +
  theme(plot.title = element_text(hjust = 0.5))
  

ggplot(data=Q3Post, aes(x=MeanNDSI, y=MeanNDVI, color=site))+
  geom_point() +
  geom_smooth(method="lm") +
  theme_few() + 
  scale_color_few() + 
  theme(legend.position="bottom") +
  labs(title="Post-Burn", x="Average NDSI", y="Average NDVI") +
  theme(plot.title = element_text(hjust = 0.5))
  

```

The averages of the burned sites were driving the positive correlation prior to the burn occurring. After the burn, these sites began to show a negative correlation between snowfall and vegetative cover, while the unburned remained fairly unaffected. When incorporating all values, the negative correlation between snowfall and vegetative cover remains consistent in all sites, even following the burn. There is a strict differentiation between the burned and unburned sites, but that is due to the difference in amount of NDVI.


4) What month is the greenest month on average? 

```{r,  echo=FALSE, warning=F, message=F}

###Creating Individual datasets###

Q4= full_long %>%
  filter(!is.na(value)) %>%
  pivot_wider(names_from= c(data),
              values_from = c(value)) %>%
  mutate(year = year(DateTime),
         month = month(DateTime, label=T)) %>%
  group_by(month) %>%
  summarise(MeanNDVI=mean(ndvi, na.rm=T))

Q4Post= full_long %>%
  filter(!is.na(value)) %>%
  pivot_wider(names_from= c(data),
              values_from = c(value)) %>%
  mutate(year = year(DateTime),
         month = month(DateTime, label=T)) %>%
  filter(year %in% c(2003:2019)) %>%
  group_by(month, site) %>%
  summarise(MeanNDVI=mean(ndvi, na.rm=T)) %>%
  filter(site %in% c("burned"))

##Visualizing with barplots##

ggplot(data=Q4, aes(x=month, y=MeanNDVI)) +
  geom_col(fill= "lightblue",
           color="blue")+
  geom_col(data= Q4[Q4$month=="Aug",],
           aes(x=month, y=MeanNDVI),
           fill = "lightgreen",
           color = "green")+
  labs(title="Average of All Plots", x="Month", y="Average NDVI") +
  theme(plot.title = element_text(hjust = 0.5))
  

ggplot(data=Q4Post, aes(x=month, y=MeanNDVI)) +
  geom_col(fill= "lightblue",
           color="blue")+
  geom_col(data= Q4Post[Q4Post$month=="Aug",],
           aes(x=month, y=MeanNDVI),
           fill = "lightgreen",
           color = "green")+
 labs(title="Post-Fire Burned Averages", x="Month", y="Average NDVI") +
  theme(plot.title = element_text(hjust = 0.5))
```


August is the greenest month across all plots and remains so in burned plots following the fire.


5) What month is the snowiest on average?

```{r,  echo=FALSE, warning=F, message=F}

##Creating average dataset##

Q5= full_long %>%
  filter(!is.na(value)) %>%
  pivot_wider(names_from= c(data),
              values_from = c(value)) %>%
  mutate(year = year(DateTime),
         month = month(DateTime, label=T)) %>%
  group_by(month) %>%
  summarise(MeanNDSI=mean(ndsi, na.rm=T))

##Plotting highest snowfall##

ggplot(data=Q5, aes(x=month, y=MeanNDSI)) +
  geom_col(fill= "lavender",
           color="red")+
  geom_col(data= Q5[Q5$month=="Jan",],
           aes(x=month, y=MeanNDSI),
           fill = "lightblue",
           color = "blue")+
 labs(title="Snowy Months", x="Month", y="Average NDSI") +
  theme(plot.title = element_text(hjust = 0.5))
```


January is the snowiest month on average across all plots (as outlined in blue).



<!--chapter:end:2-fire-data-wrangle.Rmd-->

---
title: "Snow Data Assignment: Web Scraping, Functions, and Iteration"
author: "Alex Siggers"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_file = 'index',
      output_dir='./'
    )
  })
---

```{r setup, echo=FALSE, warning=FALSE, include=FALSE}
library(rvest)
library(tidyverse)
library(lubridate)
library(readxl)

```

#Chapter 3: Snow Data Assignment for Web Scraping, Functions, & Iteration

**1. Extract the meteorological data URLs. Here we want you to use the `rvest` package to get the URLs for the `SASP forcing` and `SBSP_forcing` meteorological datasets.**

```{r, include=FALSE}
site_url <- 'https://snowstudies.org/archived-data/'

#Read the web url
webpage <- read_html(site_url)

#Extract only weblinks and then the URLs!
links <- webpage %>%
  html_nodes('a') %>%
  .[grepl('forcing',.)] %>%
  html_attr('href')

links
```


**2. Download the meteorological data. Use the `download_file` and `str_split_fixed` commands to download the data and save it in your data folder. You can use a for loop or a map function.** 

```{r, include=FALSE}

#Grab only the name of the file by splitting out on forward slashes
splits <- str_split_fixed(links,'/',8)

#Keep only the 8th column
dataset <- splits[,8] 

#generate a file list for where the data goes
file_names <- paste0('./data/3-snow-data',dataset)

for(i in 1:2){
  download.file(links[i],destfile=file_names[i])
}

downloaded <- file.exists(file_names)

evaluate <- all(downloaded)
```


**3. Write a custom function to read in the data and append a site column to the data.**

```{r, include=FALSE}

# this code grabs the variable names from the metadata pdf file
library(pdftools)
headers <- pdf_text('https://snowstudies.org/wp-content/uploads/2022/02/Serially-Complete-Metadata-text08.pdf') %>%
  readr::read_lines(.) %>%
  trimws(.) %>%
  str_split_fixed(.,'\\.',2) %>%
  .[,2] %>%
  .[1:26] %>%
  str_trim(side = "left")

headers

#This function will be utilized in a map function to read in data
#Altered to create a df from table (read.delim)
meteor_reader <- function(file){
  name=str_split_fixed(file,'_',2)[,2] %>%
    gsub('_Forcing_Data.txt','',.)
  df <- read.delim(file[1], header=F, sep="", skip=4, col.names=headers) %>%
  mutate(site=name)
}


```

**4. Use the `map` function to read in both meteorological files. Display a summary of your tibble.**

```{r, echo=FALSE, include=FALSE}
#Reading in both files using map_dfr and previous function
meteor_files = file_names

meteor_full_data = map_dfr(meteor_files, meteor_reader)

#Keeping only columns necessary for downstream
meteor_updated = select(meteor_full_data, 1:4, 7, 10, 27)

#Converting to tibble
as_tibble(meteor_updated)
```


```{r, echo=FALSE, include=TRUE}
summary(meteor_updated)
```


**5. Make a line plot of mean temp by year by site (using the `air temp [K]` variable). Is there anything suspicious in the plot? Adjust your filtering if needed.**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Creating a df of temp avgs by year and site
plotting_data = meteor_updated %>%
  group_by(year, site) %>%
  summarize(mean_temp = mean(air.temp..K.))


#Plotting full span of 2003-2011 temp avgs by site
ggplot(plotting_data, aes(x=year, y=mean_temp, color=site)) +
  geom_line()+
  scale_x_continuous(breaks = seq(2003, 2011, by=1)) +
  xlab("Year")+
  ylab("Mean Temp (K)")+
  ggtitle("SASP & SBSP Temp Comparison ('03-'11)") +
  theme(plot.title = element_text(hjust = 0.5))
```

##The temperature increases dramatically from the starting year (2003) and begins to level off & fluctuate closer to 2005. This is most likely due to the 2003 data being partial and only including winter temperatures. By filtering out 2003 and 2004, we can expect a more concise y-axis and remove outliers. 

```{r, echo=FALSE}
#Creating new plot w/o 2003 & 2004
ggplot(plotting_data[5:18,], aes(x=year, y=mean_temp, color=site)) +
  geom_line()+
  scale_x_continuous(breaks = seq(2005, 2011, by=1)) +
  xlab("Year")+
  ylab("Mean Temp (K)")+
  ggtitle("SASP & SBSP Temp Comparison ('05-'11)") +
  theme(plot.title = element_text(hjust = 0.5))
```

##The updated graph gives a far less skewed insight into the temperature range of a seven-year span. Temperatures fluctuate between a ~3 K threshold. No more visible anomalies.


**6. Write a function that makes line plots of monthly average temperature at each site for a given year. Use a for loop to make these plots for 2005 to 2010. Are monthly average temperatures at the Senator Beck Study Plot ever warmer than the Snow Angel Study Plot?**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
#Creating a df of year, month, site, temp avgs (No longer necessary)
month_plotting_data = meteor_updated %>%
  group_by(year, month, site) %>%
  summarise(mean_temp = mean(air.temp..K.))

#Generating the function
month_plot_func = function(yr, meteor_updated){
  #Consolidating df by site, month, and mean temp
  plot_data = meteor_updated %>%
    filter(year == yr) %>%
    group_by(month, site) %>%
    summarise(mean_temp = mean(air.temp..K., na.rm=T))
  
  #Creating object to plot by month and site
  x = ggplot(plot_data, aes(x=month, y=mean_temp, group=site)) +
    geom_line(aes(color=site)) +
    xlab("Month") +
    ylab("Mean Air Temp (K)") + 
    theme_classic() +
    labs(title=as.character(yr)) +
    theme(legend.position=c(0.8,0.8)) +
    theme(plot.title = element_text(hjust = 0.5))
  print(x)
}

#Last loop failed, using 2005-2010 to loop
for(i in c(2005:2010)){
  month_plot_func(i,meteor_updated)
  }

```

##The graphs display a consistently higher temperature in the Snow Angel Study Plot, although the Senator Beck Study Plot remains fairly close and mimics the curvature. The Senator Beck Study Plot is colder every year and month on average.  



**Bonus: Make a plot of average daily precipitation by day of year (averaged across all available years). Color each site.**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
daily_data = meteor_updated %>%
  group_by(site, day) %>%
  summarise(mean_precip = mean(precip..kg.m.2.s.1., na.rm=T))


ggplot(daily_data, aes(x=day, y=mean_precip, color=site)) +
  geom_line() +
    xlab("Day of Month") +
    ylab("Mean Precip (Kg*m2*s)") + 
    theme_classic() +
    ggtitle("Precip Comp by Day (All Years)") +
    theme(legend.position=c(0.8,0.8)) +
    theme(plot.title = element_text(hjust = 0.5))

```

##The average daily precipitation appears to be identical at both sites, but I could have made an error in summarizing and averaging the precipitation values. Either way, there does not appear to be any notable difference between the daily values at the two sites. 



<!--chapter:end:3-snow-data.Rmd-->

---
title: "LAGOS Spatial Analysis"
author: "Alex Siggers"
date: "`r format(Sys.time(), '%m/%d/%y')`"
output: html_document
editor_options: 
  chunk_output_type: console
knit: (function(input, ...) {
    )
    rmarkdown::render(
      input,
      output_file = 'index',
      output_dir='./'
---


```{r setup, include=FALSE}
library(tidyverse) # Tidy packages
library(sf) #Spatial package that can read and create shapefiles 
library(mapview) #Interactive maps
library(LAGOSNE) #Lots and lots of clean lake data
library(USAboundaries) #USA states and counties
library(RApiSerialize) 
library(dplyr)
library(lubridate) #For dealing with date and time
```

#Chapter 4: LAGOS Spatial Analysis Part One


```{r data-read,  include=FALSE, echo=FALSE}
# LAGOS Analysis


## Loading in data


### First download and then specifically grab the locus (or site lat longs)


# #Lagos download script
LAGOSNE::lagosne_get(dest_folder = LAGOSNE:::lagos_path())

#Load in lagos
lagos <- lagosne_load()

#Grab the lake centroid info
lake_centers <- lagos$locus

```



```{r, include=FALSE, echo=FALSE}
###Convert to spatial data

#Look at the column names

names(lake_centers)

#Look at the structure
#str(lake_centers)

#View the full dataset
#View(lake_centers %>% slice(1:100))

spatial_lakes <- st_as_sf(lake_centers,coords=c('nhd_long','nhd_lat'),
                          crs=4326) %>%
  st_transform(2163)

#Subset for plotting
subset_spatial <- spatial_lakes %>%
  slice(1:100) 

subset_baser <- spatial_lakes[1:100,]

#Dynamic mapviewer
mapview(subset_spatial)

```


```{r, include=FALSE, echo=FALSE}
### Subset to only Minnesota

states <- us_states()

#Plot all the states to check if they loaded
#mapview(states)
minnesota <- states %>%
  filter(name == 'Minnesota') %>%
  st_transform(2163)

#Subset lakes based on spatial position
minnesota_lakes <- spatial_lakes[minnesota,]

#Plotting the first 1000 lakes
minnesota_lakes %>%
  arrange(-lake_area_ha) %>%
    slice(1:1000) %>%
  mapview(.,zcol = 'lake_area_ha')
```



# In-Class work


## 1) Show a map outline of Iowa and Illinois (similar to Minnesota map upstream)

```{r, echo=TRUE}
iowa <- states %>%
  filter(name == 'Iowa') %>%
  st_transform(2163)

illinois <- states %>%
  filter(name == 'Illinois') %>%
  st_transform(2163)

mapview(iowa) +
  mapview(illinois)

```



## 2) Subset LAGOS data to these sites, how many sites are in Illinois and Iowa combined? How does this compare to Minnesota?

```{r, echo=TRUE}

iowa_lakes <- spatial_lakes[iowa,]
illinois_lakes <- spatial_lakes[illinois,]

```

-Iowa has 4,644 lakes total, and Illinois has 11,822. This combines to about ~16,500+ lakes, which is about half as many as Minnesota contains (29,038). Minnesota has a ton of lakes.


## 3) What is the distribution of lake size in Iowa vs. Minnesota?

- Here I want to see a histogram plot with lake size on x-axis and frequency on 
y axis (check out geom_histogram)

```{r, echo=TRUE, warning=FALSE, message=FALSE}
ggplot(minnesota_lakes, aes(x= lake_area_ha)) + 
  geom_histogram() + 
  scale_x_log10(labels = scales::comma) +
  ylab("Lake Area (ha)") +
  xlab("Number of lakes") +
  ggtitle("Minnesota Lakes")

ggplot(iowa_lakes, aes(x= lake_area_ha)) +
  geom_histogram() +
  scale_x_log10(labels = scales::comma) +
  ylab("Lake Area (ha)") +
  xlab("Number of lakes") +
  ggtitle("Iowa Lakes")

```

-Minnesota has many more lakes that take up over 1,000 hectares, with a few reaching 5,000 hectares. The lakes in Iowa top out around 1,100 hectares.


## 4) Make an interactive plot of lakes in Iowa and Illinois and color them by lake area in hectares

```{r, echo=TRUE}
Iowanois=rbind(iowa_lakes, illinois_lakes)

Iowanois %>%
  arrange(-lake_area_ha) %>%
    slice(1:1000) %>%
  mapview(.,zcol = 'lake_area_ha')
```


## 5) What other data sources might we use to understand how reservoirs and natural lakes vary in size in these three states? 

-Aquifer depletion/depth, precipitation, long-term climate data, and glacial movement data would help to explain these size differences. Land use change data might also help to explain the manmade lakes. 



##Part 2


```{r data-read, include=FALSE, echo=FALSE}
#Lagos download script
#lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T)

#Load in lagos
lagos <- lagosne_load()


#Grab the lake centroid info
lake_centers <- lagos$locus

# Make an sf object 
spatial_lakes <- st_as_sf(lake_centers,coords=c('nhd_long','nhd_lat'),
                          crs=4326)

#Grab the water quality data
nutr <- lagos$epi_nutr

#Look at column names
#names(nutr)
```



```{r, include=FALSE}
### Subset columns nutr to only keep key info that we want
clarity_only <- nutr %>%
  select(lagoslakeid,sampledate,chla,doc,secchi) %>%
  mutate(sampledate = as.character(sampledate) %>% ymd(.))

```



```{r, include=FALSE}
### Keep sites with at least 200 observations 

#Look at the number of rows of dataset
#nrow(clarity_only)

chla_secchi <- clarity_only %>%
  filter(!is.na(chla),
         !is.na(secchi))

# How many observatiosn did we lose?
# nrow(clarity_only) - nrow(chla_secchi)


# Keep only the lakes with at least 200 observations of secchi and chla
chla_secchi_200 <- chla_secchi %>%
  group_by(lagoslakeid) %>%
  mutate(count = n()) %>%
  filter(count > 200)


```




```{r, include=FALSE}
### Join water quality data to spatial data
spatial_200 <- inner_join(spatial_lakes,chla_secchi_200 %>%
                            distinct(lagoslakeid,.keep_all=T),
                          by='lagoslakeid')


```



```{r, include=FALSE}
### Mean Chl_a map

### Take the mean chl_a and secchi by lake

mean_values_200 <- chla_secchi_200 %>%
  # Take summary by lake id
  group_by(lagoslakeid) %>%
  # take mean chl_a per lake id
  summarize(mean_chl = mean(chla,na.rm=T),
            mean_secchi=mean(secchi,na.rm=T)) %>%
  #Get rid of NAs
  filter(!is.na(mean_chl),
         !is.na(mean_secchi)) %>%
  # Take the log base 10 of the mean_chl
  mutate(log10_mean_chl = log10(mean_chl))

#Join datasets
mean_spatial <- inner_join(spatial_lakes,mean_values_200,
                          by='lagoslakeid') 

#Make a map
mapview(mean_spatial,zcol='log10_mean_chl')
```


# Class work

## 1) What is the correlation between Secchi Disk Depth and Chlorophyll a for
sites with at least 200 observations?

- Here, I just want a plot of chla vs secchi for all sites 

```{r, echo=TRUE, include=TRUE}
ggplot(mean_values_200,
       aes(x = mean_secchi,
           y = log10_mean_chl)) +
  geom_smooth(method = "lm") +
  annotate(geom = "text",
           x = 7, 
           y = 3.5,
           label = paste0("r = ", round(cor(mean_values_200$mean_secchi, 
                                            mean_values_200$log10_mean_chl), 3))) +
  xlab("Avg Secchi Disk Depth") +
  ylab("Chlorophyll A (Log10 Transformed)") +
  geom_point(size = 2) +
  theme_light()

```


## Why might this be the case? 
-A higher Chlorophyll A content is typically correlated with increased algae/phytoplankton levels, which would limit the visibility of the lake at depth. In other words, the Secchi Disk will not be visible deeper in the lake when there is an increased amount of biomass (Chlorophyll A) on the surface.

## 2) What states have the most data? 

### 2a) First you will need to make a lagos spatial dataset that has the total 
number of counts per site.

```{r, echo=TRUE, include=TRUE}

NutData <- lagos$epi_nutr

counts <- NutData %>%
  group_by(lagoslakeid) %>%
  mutate(count = n()) %>% 
  inner_join(spatial_lakes, ., by='lagoslakeid') 

```


### 2b) Second, you will need to join this point dataset to the us_boundaries 
data. 

```{r, echo=TRUE, include=TRUE}

states=us_states()

JointData = st_join(states, counts)

```


### 2c) Then you will want to group by state and sum all the observations in that
state and arrange that data from most to least total observations per state. 

```{r, echo=TRUE, include=TRUE}

StateLakes <- aggregate(count ~ state_name, data = JointData, FUN = sum)
StateLakes <- arrange(StateLakes, desc(count))

```

-Minnesota, Wisconsin, Michigan, Maine, and Vermont have the most data out of the 21 states with available data. These states have been the most shaped by natural glacial activity, hence there are many lakes within each. 


##3 Is there a spatial pattern in Secchi disk depth for lakes with at least 200 
observations?

```{r, echo=TRUE, include=TRUE}

mapview(mean_spatial,
        zcol = "mean_secchi")

```


-It appears that most of the lakes in the middle-Midwestern United States have a shallowed mean Secchi Disk Depth than in the northeastern portion of the country. This could be related to nutrient runoff and leaching from agricultural production, but that is extrapolation. However, this is the most notable trend as these lakes display a typical mean depth between 1-4m. 


<!--chapter:end:4-LAGOS-analysis.Rmd-->

---
title: "Lake Water Quality Analysis"
author: "Alex Siggers"
date: "`r format(Sys.time(), '%m/%d/%y')`"
output: html_document
editor_options: 
  chunk_output_type: console
knit: (function(input, ...) {
    )
    rmarkdown::render(
      input,
      output_file = 'index',
      output_dir='./'
---



```{r setup, include=FALSE}
library(tidyverse) # Tidy packages
library(sf) #Spatial package that can read and create shapefiles 
library(mapview) #Interactive maps
library(LAGOSNE) #Lots and lots of clean lake data
library(USAboundaries) #USA states and counties
library(lubridate) #For dealing with date and time
```

#Chapter 5: LAGOS Water Quality Analysis Part 2


```{r data-read, include=FALSE, echo=FALSE, warning=FALSE}
#Lagos download script
#lagosne_get(dest_folder = LAGOSNE:::lagos_path(),overwrite=T)

#Load in lagos
lagos <- lagosne_load()


#Grab the lake centroid info
lake_centers <- lagos$locus

# Make an sf object 
spatial_lakes <- st_as_sf(lake_centers,coords=c('nhd_long','nhd_lat'),
                          crs=4326)

#Grab the water quality data
nutr <- lagos$epi_nutr

#Look at column names
#names(nutr)
```



```{r, include=FALSE}
### Subset columns nutr to only keep key info that we want
clarity_only <- nutr %>%
  select(lagoslakeid,sampledate,chla,doc,secchi) %>%
  mutate(sampledate = as.character(sampledate) %>% ymd(.))

```



```{r, include=FALSE}
### Keep sites with at least 200 observations 

#Look at the number of rows of dataset
#nrow(clarity_only)

chla_secchi <- clarity_only %>%
  filter(!is.na(chla),
         !is.na(secchi))

# How many observatiosn did we lose?
# nrow(clarity_only) - nrow(chla_secchi)


# Keep only the lakes with at least 200 observations of secchi and chla
chla_secchi_200 <- chla_secchi %>%
  group_by(lagoslakeid) %>%
  mutate(count = n()) %>%
  filter(count > 200)


```




```{r, include=FALSE}
### Join water quality data to spatial data
spatial_200 <- inner_join(spatial_lakes,chla_secchi_200 %>%
                            distinct(lagoslakeid,.keep_all=T),
                          by='lagoslakeid')


```



```{r, include=FALSE}
### Mean Chl_a map

### Take the mean chl_a and secchi by lake

mean_values_200 <- chla_secchi_200 %>%
  # Take summary by lake id
  group_by(lagoslakeid) %>%
  # take mean chl_a per lake id
  summarize(mean_chl = mean(chla,na.rm=T),
            mean_secchi=mean(secchi,na.rm=T)) %>%
  #Get rid of NAs
  filter(!is.na(mean_chl),
         !is.na(mean_secchi)) %>%
  # Take the log base 10 of the mean_chl
  mutate(log10_mean_chl = log10(mean_chl))

#Join datasets
mean_spatial <- inner_join(spatial_lakes,mean_values_200,
                          by='lagoslakeid') 

#Make a map
mapview(mean_spatial,zcol='log10_mean_chl')
```


# Class work

## 1) What is the correlation between Secchi Disk Depth and Chlorophyll a for sites with at least 200 observations?

- Here, I just want a plot of chla vs secchi for all sites 

```{r, echo=TRUE, include=TRUE}
ggplot(mean_values_200,
       aes(x = mean_secchi,
           y = log10_mean_chl)) +
  geom_smooth(method = "lm") +
  annotate(geom = "text",
           x = 7, 
           y = 3.5,
           label = paste0("r = ", round(cor(mean_values_200$mean_secchi, 
                                            mean_values_200$log10_mean_chl), 3))) +
  xlab("Avg Secchi Disk Depth") +
  ylab("Chlorophyll A (Log10 Transformed)") +
  geom_point(size = 2) +
  theme_light()

```


## Why might this be the case? 


-A higher Chlorophyll A content is typically correlated with increased algae/phytoplankton levels, which would limit the visibility of the lake at depth. In other words, the Secchi Disk will not be visible deeper in the lake when there is an increased amount of biomass (Chlorophyll A) on the surface.

## 2) What states have the most data? 

### 2a) First you will need to make a lagos spatial dataset that has the total number of counts per site.

```{r, echo=TRUE, include=TRUE}

NutData <- lagos$epi_nutr

counts <- NutData %>%
  group_by(lagoslakeid) %>%
  mutate(count = n()) %>% 
  inner_join(spatial_lakes, ., by='lagoslakeid') 

```


### 2b) Second, you will need to join this point dataset to the us_boundaries data. 

```{r, echo=TRUE, include=TRUE}

states=us_states()

JointData = st_join(states, counts)

```


### 2c) Then you will want to group by state and sum all the observations in that state and arrange that data from most to least total observations per state. 

```{r, echo=TRUE, include=TRUE}

StateLakes <- aggregate(count ~ state_name, data = JointData, FUN = sum)
StateLakes <- arrange(StateLakes, desc(count))

```


-Minnesota, Wisconsin, Michigan, Maine, and Vermont have the most data out of the 21 states with available data. These states have been the most shaped by natural glacial activity, hence there are many lakes within each. 


##3 Is there a spatial pattern in Secchi disk depth for lakes with at least 200 
observations?

```{r, echo=TRUE, include=TRUE}

mapview(mean_spatial,
        zcol = "mean_secchi")

```


-It appears that most of the lakes in the middle-Midwestern United States have a shallowed mean Secchi Disk Depth than in the northeastern portion of the country. This could be related to nutrient runoff and leaching from agricultural production, but that is extrapolation. However, this is the most notable trend as these lakes display a typical mean depth between 1-4m. 


<!--chapter:end:5-Lake-wq-analysis.Rmd-->

---
title: "Weather and Corn Yield Regressions"
author: "Alex Siggers"
date: "`r Sys.Date()`"
output: html_document
knit: (function(input, ...) {
    rmarkdown::render(
      input,
      output_file = 'index',
      output_dir='./'
    )
  })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(R.matlab)
library(rnassqs)
library(ggplot2)
```

#Chapter 6: Weather & Crop Regressions

```{r tmax data, echo=FALSE, include=FALSE, message=FALSE, warning=FALSE}
# daily max temperature
# dimensions: counties x days x years
prism <- readMat("data/6-Weather-corn-regression/prismiowa.mat")

# look at county #1
t_1981_c1 <- prism$tmaxdaily.iowa[1,,1]
t_1981_c1[366]
plot(1:366, t_1981_c1, type = "l")
ggplot() +
  geom_line(mapping = aes(x=1:366, y = t_1981_c1)) +
  theme_bw() +
  xlab("day of year") +
  ylab("daily maximum temperature (°C)") +
  ggtitle("Daily Maximum Temperature, Iowa County #1")

```


```{r tidying up, echo=FALSE, include=FALSE, message=FALSE}
# assign dimension names to tmax matrix
dimnames(prism$tmaxdaily.iowa) <- list(prism$COUNTYFP, 1:366, prism$years)
# converted 3d matrix into a data frame
tmaxdf <- as.data.frame.table(prism$tmaxdaily.iowa)
# relabel the columns
colnames(tmaxdf) <- c("countyfp","doy","year","tmax")
tmaxdf <- tibble(tmaxdf)
```

```{r temp trends, echo=FALSE, include=FALSE, message=FALSE}
tmaxdf$doy <- as.numeric(tmaxdf$doy)
tmaxdf$year <- as.numeric(as.character(tmaxdf$year))
winnesummer <- tmaxdf %>%
  filter(countyfp==191 & doy >= 152 & doy <= 243) %>%
  group_by(year) %>%
  summarize(meantmax = mean(tmax))
ggplot(winnesummer, mapping = aes(x = year, y = meantmax)) +
  geom_point() +
  theme_bw() +
  labs(x = "year", y = "Tmax (°C)") +
  geom_smooth(method = lm)
lm_summertmax <- lm(meantmax ~ year, winnesummer)
summary(lm_summertmax)
```


```{r winter temps, echo=FALSE, include=FALSE, message=FALSE}
winnewinter <- tmaxdf %>%
  filter(countyfp==191 & doy <= 59 | doy >= 335 & !is.na(tmax)) %>%
  group_by(year) %>%
  summarize(meantmax = mean(tmax))
ggplot(winnewinter, mapping = aes(x = year, y = meantmax)) +
  geom_point() +
  theme_bw() +
  labs(x = "year", y = "Tmax (°C)") +
  geom_smooth(method = lm)
lm_wintertmax <- lm(meantmax ~ year, winnewinter)
summary(lm_wintertmax)
```



```{r quadratic temp trend, echo=FALSE, include=FALSE, message=FALSE}
winnewinter$yearsq <- winnewinter$year^2
lm_wintertmaxquad <- lm(meantmax ~ year + yearsq, winnewinter)
summary(lm_wintertmaxquad)
winnewinter$fitted <- lm_wintertmaxquad$fitted.values
ggplot(winnewinter) +
  geom_point(mapping = aes(x = year, y = meantmax)) +
  geom_line(mapping = aes(x = year, y = fitted)) +
  theme_bw() +
  labs(x = "year", y = "tmax")
```


```{r yield download, echo=FALSE, include=FALSE, message=FALSE}
# set our API key with NASS
nassqs_auth(key = "9DB94D80-F55F-379D-AF64-B3EC20632EB0")

# parameters to query on 
params <- list(commodity_desc = "CORN", util_practice_desc = "GRAIN", prodn_practice_desc = "ALL PRODUCTION PRACTICES", year_GE = 1981, state_alpha = "IA")

# download
cornyieldsall <- nassqs_yields(params)

cornyieldsall$county_ansi <- as.numeric(cornyieldsall$county_ansi)
cornyieldsall$yield <- as.numeric(cornyieldsall$Value)

# clean and filter this dataset
cornyields <- select(cornyieldsall, county_ansi, county_name, yield, year) %>%
  filter(!is.na(county_ansi) & !is.na(yield))
cornyields <- tibble(cornyields)

view(cornyields)
```

## Assignment

### Question 1a: Extract Winneshiek County corn yields, fit a linear time trend, make a plot. Is there a significant time trend?

```{r, echo=TRUE, message=FALSE, warning=FALSE}
#1a
WinneshiekCorn <- cornyields %>%
  filter(county_name == "WINNESHIEK")

ggplot(WinneshiekCorn, mapping = aes(x = year, y = yield)) +
  geom_point() +
  theme_bw() +
  labs(x = "Year", y = "Corn Yield") +
  geom_smooth(method = lm)

WinCornLm <- lm(yield ~ year, WinneshiekCorn)
summary(WinCornLm)
```

-##There is a very significant linear increase in corn yield with year. The linear model provides evidence, and the scatterplot visualizes the trend. ##


### Question 1b: Fit a quadratic time trend (i.e., year + year^2) and make a plot. Is there evidence for slowing yield growth? 

```{r, echo=TRUE, message=FALSE, warning=FALSE}
#1b
WinneshiekCorn$yearsq <- WinneshiekCorn$year^2

lm_wincornquad <- lm(yield ~ year + yearsq, WinneshiekCorn)
summary(lm_wincornquad)

WinneshiekCorn$fitted <- lm_wincornquad$fitted.values

ggplot(WinneshiekCorn) +
  geom_point(mapping = aes(x = year, y = yield)) +
  geom_line(mapping = aes(x = year, y = fitted)) +
  theme_bw() +
  labs(x = "Year", y = "Corn Yield")
```

-##The evidence seems to suggest that the corn yield is only growing, seemingly exponentially under the quadratic fit. ##


### Question 2 -- Time Series: Let's analyze the relationship between temperature and yields for the Winneshiek County time series. Use data on yield and summer avg Tmax. Is adding year or Tmax^2 to your model helpful? Make a plot and interpret the results.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
WinTime = merge(WinneshiekCorn, winnesummer, by="year")

YieldTemplm=lm(yield~meantmax, WinTime)
summary(YieldTemplm)

ggplot(WinTime, mapping = aes(x = meantmax, y = yield)) +
  geom_point() +
  theme_bw() +
  labs(x = "Average Summer Temp", y = "Corn Yield") +
  geom_smooth(method = lm)
```

-##Visually, there appears to be a negative correlation between average maximum temperature and yield, but the model does not provide strong evidence for such a relationship. Adding year to the model provides significant evidence for a relationship in that regard, but we are note exactly looking for that relationship here. ##


### Question 3 -- Cross-Section: Analyze the relationship between temperature and yield across all counties in 2018. Is there a relationship? Interpret the results.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
#Q3
rename(tmaxdf, "county_ansi" = "countyfp")
names(tmaxdf)[names(tmaxdf)=="countyfp"] <- "county_ansi"

TempData2018 <- tmaxdf %>%
  filter(doy >= 152 & doy <= 243) %>%
  filter(year == "2018") %>%
  group_by(county_ansi)

Data2018 <- cornyields %>%
  filter(year == "2018")
  
TD2018 <- merge(TempData2018, Data2018, by="county_ansi")
  
  

lm2018 <- lm(yield~tmax, TD2018)
summary(lm2018)

ggplot(TD2018, mapping = aes(x = tmax, y = yield)) +
  geom_point() +
  theme_bw() +
  labs(x = "Max Summer Temp", y = "Corn Yield") +
  geom_smooth(method = lm)

```

-##These results appear to mimic the trend of the summer Winnishiek regression, which is minimal. There is a slight decreasing trend, but overall very little evidence of a trend at all. ##

### Question 4 -- Panel: One way to leverage multiple time series is to group all data into what is called a "panel" regression. Convert the county ID code ("countyfp" or "county_ansi") into factor using as.factor, then include this variable in a regression using all counties' yield and summer temperature data. How does the significance of your temperature coefficients (Tmax, Tmax^2) change? Make a plot comparing actual and fitted yields and interpret the results of your model.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(gridExtra)
#Q4
#Converting to factors
cornyields$county_ansi=as.factor(cornyields$county_ansi)
cornyields$county_name=as.factor(cornyields$county_name)
tmaxdf$county_ansi=as.factor(tmaxdf$county_ansi)

#Creating tmax and tmax^2 df
MaxTemps <- tmaxdf %>%
  filter(doy >= 152 & doy <= 243) %>%
  group_by(year, county_ansi) %>%
  summarize(meantmax = mean(tmax))

MaxTemps$county_ansi=as.factor(MaxTemps$county_ansi)

#Creating master df
MaxTempYield = merge(MaxTemps, cornyields) %>%
  mutate(yearsq=year^2,
         yieldsq=yield^2,
         tmaxsq=meantmax^2)

#Creating models for each factor
tempquadr = lm(yield~meantmax + tmaxsq, MaxTempYield)
summary(tempquadr)
MaxTempYield$tempsqfit <- tempquadr$fitted.values

yearlm = lm(yield~year, MaxTempYield)
summary(yearlm)
MaxTempYield$yearfit <- yearlm$fitted.values

countylm = lm(yield~county_name, MaxTempYield)
summary(countylm)
MaxTempYield$countyfit <- countylm$fitted.values

allquadr = lm(yield ~ year + meantmax + tmaxsq + county_name, MaxTempYield)
summary(allquadr)
MaxTempYield$allfit <- allquadr$fitted.values

summary(MaxTempYield)
```


```{r, echo=TRUE, message=FALSE, warning=FALSE}
#Creating visuals'
grid.arrange(
top = "Individual factors regressed against yield for all counties",
layout_matrix = rbind(c(1, 2, 3),c(4, 4, 4)),
ggplot(MaxTempYield) +
  geom_point(mapping = aes(x = yield, y = tempsqfit), alpha = 0.2) +
  geom_line(mapping = aes(x = yield, y = yield), linetype = "solid", size=0.5) +
  labs(x = "Yield (Actual)", y = "Yield (Fitted)", title= "Quad Temp",) +
  theme_gray()
,
ggplot(MaxTempYield) +
  geom_point(mapping = aes(x = yield, y = yearfit), alpha = 0.2) + 
  geom_line(mapping = aes(x = yield, y = yield), linetype = "solid", size=0.5) +
  labs(x = "Yield (Actual)", y = "Yield (Fitted)", title= "Year") +
  theme_gray()
,
ggplot(MaxTempYield) +
  geom_point(mapping = aes(x = yield, y = countyfit), alpha = 0.2) + 
  geom_line(mapping = aes(x = yield, y = yield), linetype = "solid", size=0.5) +
  labs(x = "Yield (Actual)", y = "Yield (Fitted)", title= "County Model") +
  theme_gray()
,
ggplot(MaxTempYield) +
  geom_point(mapping = aes(x = yield, y = allfit), alpha = 0.2) + 
  geom_line(mapping = aes(x = yield, y = yield), linetype = "solid", size=1.5) +
  labs(x = "Yield (Actual)", y = "Yield (Fitted)", title= "Combined Model", subtitle="Quadratic Temp, Year, County") +
  theme_gray()
)

```

-##The fit of years to yield (although not quadratic) provides the best fit to explain the correlation. The temperature and county models have very limited predictive power (though the temperature model is better than the county model). The combined model creates the best prediction visually. ##

### Question 5 -- Soybeans: Download NASS data on soybean yields and explore either a time series relationship for a given county, the cross-sectional relationship for a given year, or a panel across all counties and years.

```{r, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#Reading in & downloading data
soypar <- list(
  commodity_desc = "SOYBEANS", statisticcat_desc= "YIELD", 
  prodn_practice_desc = "ALL PRODUCTION PRACTICES", year__GE = 1981, state_alpha = "IA")

soyyield<- nassqs_yields(soypar)
```


```{r, echo=TRUE, message=FALSE, warning=FALSE}
soyyield$county_ansi <- as.numeric(soyyield$county_ansi)
soyyield$yield <- as.numeric(soyyield$Value)

# clean and filter this dataset
soyyields <- select(soyyield, county_ansi, county_name, yield, year) %>%
  filter(!is.na(county_ansi) & !is.na(yield))
soyyields <- tibble(soyyields)


#Subsetting a single county
CherokeeSoy <- soyyields %>%
  filter(county_name == "CHEROKEE")

ggplot(CherokeeSoy, mapping = aes(x = year, y = yield)) +
  geom_point() +
  theme_bw() +
  labs(x = "Year", y = "Soy Yield") +
  geom_smooth(method = lm)

CherSoyLm <- lm(yield ~ year, CherokeeSoy)
summary(CherSoyLm)

```

-There is a very evident positive correlation between time and soy yield in Cherokee county, which is supported by the linear model as well.


<!--chapter:end:6-Weather-corn-regression.Rmd-->

